#notes
#using [] in dataframe to filter columns
#using subsetting using () for multiple conditions 
pd.show_versions(as_json=False) # get the version of all packages

#importing the dataset
hrdata=pd.read_csv(r'C:\pyworkspace\datasets\HR DataSets\HRDataset_v13.csv') #importing csv
hrdata=pd.read_excel(r'C:\pyworkspace\datasets\HR DataSets\EmploymentStatus.xslx') #importing excel
df = pd.concat(pd.read_excel(workbook_url, sheet_name=None), ignore_index=True) #return a single pandas dataframe that combines the data in each Excel worksheet if only all the columns are same

#exporting
hrdata.to_excel("ds.xlsx",index=False)  #exporting and removing the index.

#dataframe common operations
prodstaff.columns  #description the columns
prodstaff.info()   #detail information of the variable with observations.
prodstaff.head()  #top 10 observations
prodstaff.head(20)  #top 20 observations
prodstaff.tail()  #last 10 observations
prodstaff.tail(20)  #last 20 observations

#casting data type
hrdata['purchase'].astype(str).astype(int) # converting from object to other data type like int
hrcore['Employee Number']=hrcore['Employee Number'].astype(float) # converting from object to float
hrcore['Employee Number']=hrcore['Employee Number'].astype(int)	  # converting from object to int

#pandas options
pd.set_option('max_rows',10) #Maxium rows displayed in this session
pd.set_option('max_columns',1111)   #Maxium Columns displayed in this session
pd.set_option('max_colwidth',3)   #Maxium Columns with displayed in this session
pd.set_option('precision',3)   # decimal for more options  https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html#available-options

#data retriving 
prodstaff  #viewing all the columns in datasets
hrdata.Employee_Name # displaying one column
hrdata['Employee_Name'] # displaying one column
hrdata[['Employee_Name','Sex','ManagerName','EmploymentStatus']] # display more then one columnn using brackets

#Filtering or subsetting WHERE Caluse
hrdata['Department'].unique() # unique name of the department using brackets
hrdata.Department.unique() #another way to get the unique name
hrdata[hrdata.Department=='Sales'] # fiter based on one conditions
hrdata[hrdata.Department=='Sales'].Employee_Name # fiter only one conditions using brackets
hrdata[(hrdata.Department=='Sales') & (hrdata.EmploymentStatus == 'Active')] # fiter more than one conditons using paratheses
hrdata[hrdata['Department']=='Sales'].Employee_Name # another way of above
hrdata[(hrdata.Department=='Sales') & (hrdata.EmploymentStatus == 'Active')][['Employee_Name','Sex']]#return  based on two conditions and display two columns
hrcore_1=hrcore[hrcore['Employee Name'].str.startswith(r'Ga')].copy(deep=True) #pattern matching for starting value
hrcore_1=hrcore[hrcore['Employee Name'].str.startswith(r'Ga',,na=False)]) #ignore Nan..
hrcore_1=hrcore[hrcore['Employee Name'].str.contains(r'gan|fen', case=False, na=False)] #pattern matching on containing value

#Filtering or subsetting IN/NOT IN
hrdata[(hrdata.Department.isin(['Sales','Software Engineering']))][['Employee_Name','Department']] # in operator .isin() function
hrdata[(~hrdata.Department.isin(['Sales','Software Engineering']))][['Employee_Name','Department']] # not in operator as ~

# sorting the data 
hrcore_2=hrcore_1.sort_values('Date of Hire',ascending=True) #setting to ascending/descending for one column
hrcore_2=hrcore_1.sort_values(['Age','Date of Termination'],ascending=[False,True]) #setting to ascending/descending for more than column
hrcore_1[['Employee Name','Age','Date of Hire']].sort_values(['Date of Hire'],ascending=[False]) #setting to ascending/descending for displaying column
hrdata[(hrdata.Department=='Sales')].sort_values('Employee_Name') # default by ascending based on conditions
hrdata[['Employee_Name','EmpID','State','Department','ManagerName','RecruitmentSource']].sort_values(['Department','ManagerName','State'],ascending=[True,False,False]) #sorting more then values with more than one order like ascending, descending

#grouping the datasets
hrdata.groupby(['Department']).size()#groupby by one column
hrdata.groupby(['Department','State','Sex']).size()#groupby by more than one column

#to_frame convert to series object to dataframe and reset_index to reset the row number
hrdata.groupby(['Department','State']).size().to_frame('size').reset_index().sort_values(['Department','State'],ascending=[False,False]) #groupby by more than one column

#having
hrdata.groupby('Department').filter(lambda g: len(g) < 100).groupby('Department').size().sort_values(ascending=True) 

#top 10 records
star=hrdata.groupby('count_post').size().to_frame('count_post').reset_index()
star.nlargest(11,'count_dept')
star.nlargest(100,'count_post').tail(10)

#aggregate functions
dar=star['count_post']
dar.agg({'count_post': ['min', 'max', 'mean', 'median']})

#join
empjoin_dep=empdetails.merge(departments,left_on='DeptID',right_on='DeptID',how='inner').drop(columns=['DeptID']) # inner join 
empjoin_gen=empjoin_dep.merge(MaritalStatus,left_on='MaritalStatusID',right_on='MaritalStatusID',how='left').drop(columns=['MaritalStatusID'])  # left join 
empjoin_gen.merge(EmploymentStatus,left_on='EmpStatusID',right_on='EmpStatusID',how='right').drop(columns=['EmpStatusID'])   # right join 
[['Employee_Name','Department','MaritalDesc','EmploymentStatus']]

#UNION ALL and UNION
emp=empdetails[['Employee_Name','EmpID','DateofHire','TermReason','RecruitmentSource']]
emp1=emp[(emp.RecruitmentSource=='Website Banner Ads')]
emp2=emp[(emp.RecruitmentSource=='Internet Search')]
emp_union= pd.concat([emp1,emp2]) #union all
emp_union= pd.concat([emp1,emp2]).drop_duplicates('RecruitmentSource') #union i,e to remove duplicates

#adding ,renaming,replacing a value and dropping the column
import numpy as np
hrdataset[['Employee_Name','EmpID','GenderID']]
hrdataset['Sex']= np.where(hrdataset['GenderID']==1, 'Male', 'Female') #use for if conditions else sort of communication
rankings_pd.rename(columns = {'test':'TEST'}, inplace = True) # renaming the value
rankings_pd.rename(columns = {'test':'TEST', 'odi':'ODI', 't20':'T20'}, inplace = True)  # renaming the multi columns
hrdata['Sex']=hrdata['GenderID'].replace([0,1],['Male','Female']) # renaming the value of that column

#null handling
hrcore['Date of Termination'].isnull().sum() #count of null values
hrcore_1=hrcore.dropna()  #drop all the missing values(NaN, pd.NaT, None) in a datframe
hrcore1=hrcore.dropna(subset=['Date of Termination']) # drop missing value specfic to that column
hr.isnull() #isnull() function this function return dataframe of Boolean values which are True for NaN values
pd.isnull(hr['Date of Termination']) #the above only for specific column
hrcore_3=hrcore_2[~hrcore_2['Date of Termination'].isnull()] # Filtering the non null value
hrcore_3=hrcore_2[hrcore_2['Date of Termination'].isnull()] #  Filtering the null value
hr.notnull() #notnull() function this function return dataframe of Boolean values which are False for NaN values
pd.notnull(hr['Date of Termination'])
hr.fillna(0) #fillna()
dframe['Product'] = dframe['Product'].fillna('') # replace null to blank
hr.fillna(method ='pad') #fillna() # filling a missing value with  previous ones
hr.fillna(method ='bfill') #fillna() filling  null value using fillna() function	

#Date handling
tdate=pd.datetime.now().strftime("%d/%m/%Y") #getting today date
hrcore['DOB']= pd.to_datetime(hrcore.DOB) # convert the object to date
hrcore['DOB1']= hrcore['DOB'].dt.strftime('%d-%m-%Y') # convert the yyyy-mm-dd to dd-mm-yyyy
capsys_1['TO Approval']=pd.to_datetime(capsys_1['TO Approval'],errors='coerce') # this errors helps to convert bad data into
hrcore['Date of Termination']+ pd.DateOffset(days=4) # adding the days
pd.to_datetime(hrcore['Date'], utc=False) #remove the time components

#common erros 
df1['A'].copy(deep=True) pd.options.mode.chained_assignment = None https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas

#pivoting
pd.pivot_table(production_staff,index='Department') # using index to transpose the LHS side 
pd.pivot_table(production_staff,index=['Department','Manager Name']) # using multiple index 
pd.pivot_table(production_staff,index=['Department','Manager Name'],values=['Abutments/Hour Wk 1']) # using value for aggreations, its default average
pd.pivot_table(production_staff,index=['Department','Manager Name'],values=['Abutments/Hour Wk 1'],aggfunc=np.sum) # using aggrefun for specifying function.
pd.pivot_table(production_staff,index=['Department','Manager Name'],values=['Abutments/Hour Wk 1'],aggfunc=[np.mean,len]) # using aggrefun for more than one function
pd.pivot_table(production_staff,index=['Department','Manager Name'],values=['Abutments/Hour Wk 1'],columns=['Position'],aggfunc=[np.count_nonzero])# using columns options to segament values
pd.pivot_table(production_staff,index=['Department','Manager Name','Employee Name'],values=['Abutments/Hour Wk 1','Abutments/Hour Wk 2'],columns=['Position'],aggfunc=[np.sum],fill_value=0)# use fill_value options to fille NaN to zero
pd.pivot_table(production_staff,index=['Department','Manager Name','Employee Name'],values=['Abutments/Hour Wk 1','Abutments/Hour Wk 2'],aggfunc=[np.sum,np.mean],fill_value=0) # remove the column
pd.pivot_table(production_staff,index=['Department','Manager Name','Employee Name'],values=['Abutments/Hour Wk 1','Abutments/Hour Wk 2'],aggfunc=[np.sum,np.mean],fill_value=0,margins=True) # adding margins. 
pd.pivot_table(production_staff,index=['Department','Manager Name','Employee Name'],values=['Abutments/Hour Wk 1','Pay',],aggfunc={'Pay':np.sum,'Abutments/Hour Wk 1':np.count_nonzero},fill_value=0) # using values for applying aggre for individual values.
pd.pivot_table(production_staff,index=['Department','Manager Name','Employee Name'],values=['Abutments/Hour Wk 1','Pay',],aggfunc={'Pay':np.sum,'Abutments/Hour Wk 1':[np.sum,np.count_nonzero]},fill_value=0) # list of aggfunctions to apply to each value too:
#sorting by values
df = pd.DataFrame.from_dict([{'Country': 'A', 'Year':2012, 'Value': 20, 'Volume': 1}, {'Country': 'B', 'Year':2012, 'Value': 100, 'Volume': 2}, {'Country': 'C', 'Year':2013, 'Value': 40, 'Volume': 4}])
df_pivot = pd.pivot_table(df, index=['Country'], columns = ['Year'],values=['Value'], fill_value=0)
df = df_pivot.reindex(df_pivot['Value'].sort_values(by=2012, ascending=False).index)