Intro Data Pup-IMPDB and EXPDB
==============================
-It is a new features of Oracle DB 10g that provides high speed, parallel, bulk data and metadata movement of Oracle DB contents
-this is a new technology enables transferring data from one DB to another using the following utilities.
1)Data Pump Export[Invoked using EXPDP]
2)Data Pump Import[Invoked using IMPDP]

-This utilities are similar in terms of look and feel with the pre oracle 10g IMPort and EXPort utitlites, however technically these are completely different
-*Dump files generated by the ogrinal export utility [EXP] cannot be imported using the new Data Pump Import utility [IMPDP] and vice-versa
-*Data Pump Export[EXPDP] and data Pump Import[IMPDP] are server based rather than client-based as in the case with the orginal Export[EXP] and Import [IMP].
-Data Pump Export and Import tools have vastly improved performance and greatly enhanced functionality such as flexible object selection and better monitoring and control of export and import jobs.
-Oracle recommends using these new Data Pump Export and import clients rather then orginal export[EXP] and Import [IMP] clients
-these are server side utilities,Dump,Log and SQL files are accessed relative to the server-based directory paths.

Creating Directory Objects
==========================
-Data Pump requires that directory objects are mapped and a file system directory is specified when invoking the data pump Import and Export Utility.

Creating the directory objects
	CREATE DIRECTORY MyBackups AS '/MyData/MyBackups';

-After directory object is created the user who will perform the data pump export and import should be granted read and write permissions on the directory objects.

Ensure that appropriate permissions are set for the /MyData/MyBackups directory

	GRANT READ,WRITE ON DIRECTORY MyBackups TO ganesh,hr;

Invoking Data Pump Export
=========================
-Data Pump Export is a utitlity for unloading data and metadata into a set of OS files called a dump file set. the Dump file set can be copied to another system and loaded by the Data Pump Import utility
-The Dump file set is made up of one or more disk files that contains table data, DB object metadata and control informations
-The files are written in a property, binary format
-The Data pump export is invoked using the EXPDP command in the command line.
-Export parameter can be specified directly in the command line. Data Pump export supports the following modes
1)Full Export mode
2)Schema Export Mode
3)Table Export Mode

1)Full Export mode
*----------------*
-A full export is specified using the FULL parameter.
-In a full DB export, the entire DB is unloaded.
-This mode requires that the user attempting to use this mode has the EXP_FULL_DATABASE role assigned.

Sytnax
	EXPDP SYSTEM/<Password> DIRECTORY=<DirectoryName> DUMPFILE=<fileName>.dmp FULL=y LOGFILE=<fileName>.log;


2)Schema Export mode
*------------------*
-The schema export mode is invoked using the SCHEMAS parameter
-If the EXP_FULL_DATABASE role is not assigned to the user then the user performing the export can only export the schema that belongs to that user
-If the EXP_FULL_DATABASE role is assigned to that user then several schemas can be exported in one go.

Sytnax
	EXPDP SYSTEM/<Password> DIRECTORY=<DirectoryName> DUMPFILE=<fileName>.dmp SCHEMAS=<schemaName>;


3)Table Export mode
*------------------*	
-This export mode is specified using the TABLES parameter. in this mode, only the specified tables, partitions and their dependents are exported
-if the EXP_FULL_DATABASE role is not assigned to the user then the user performing the export can only export the tables in the schema that belongs to the users.

Syntax
	EXPDP <schemaName>/<Password> DIRECTORY=<DirectoryName> DUMPFILE=<fileName>.dmp TABLES=<tableName1>,<TableName2>;


Invoking Data Pump Import
=========================
-Data Pump Import is utility for loading an export dump file set into a target system.
-The Dump file set is made up of one or more disk files that contains table data, DB objects metadata and control informations.
-the files are written by the Data Pump Export Utility in a properietary,binary format.
-During an import operations, the data pump import utility uses these files to locate each DB object in the dump file set.
-it is invoked using IMPDP command in the command line .
-Import parameter can be specified directly in the command line. Data Pump Import supports the following modes
1)Full Export mode
2)Schema Export Mode
3)Table Export Mode

1)Full import mode
*----------------*
-A full import is specified using the FULL parameter.
-the full import mode load the entire contents of the source[held in the export dump file] to the target DB.
-this requires the IMP_FULL_DATABASE role to the granted to the user performing this activity on the target DB.

Sytnax
	IMPDP SYSTEM/<Password> DIRECTORY=<DirectoryName> DUMPFILE=<fileName>.dmp FULL=y LOGFILE=<fileName>.log;


2)Schema Export mode
*------------------*
-the schema import mode is invoked using the SCHEMAS parameter
-Only the contents of the specified are loaded into the target DB.
-the Source Dump file can be a full, schema-mode,table or tablespace mode export files.
-If the IMP_FULL_DATABASE role is assigned to the user performing this activity then a list of schemas can be specified to load into target DB.

Sytnax
	IMPDP <schemaName>/<Password> DIRECTORY=<DirectoryName> DUMPFILE=<fileName>.dmp LOGFILE=<fileName>.log SCHEMAS=<schemaName>;

3)Table Export mode
*------------------*	
-this import mode is specified using the TABLES parameter.
-In this mode, only the specified tables, partitions and their dependents are imports
-If the IMP_FULL_DATABASE role is not assigned to the user performing this activity, then the user can import only tables from the schema that belongs to the user.

Syntax
	IMPDP <schemaName>/<Password> DIRECTORY=<DirectoryName> DUMPFILE=<fileName>.dmp TABLES=<tableName1>,<TableName2>;


Using Enterprise Manager(EM)
============================
-Oracle Data Pump Import and Export can be accessed using Enterprise manager.

Oracle 11g Steps

Data Pump Export
----------------
1)To export a schema using EM, open a web browser point it to 
	https://<hostname>:1158/em
2)Ensure that DBConsole service is switched on for the Enterprise manager to be served
3)Log in SYSTEM/<password>
4)Click on Data Movement link. this displays the login entry screen
5)Click on Export to Export Files link.Select the Mode, for Example for export tables, select the Tables Button, Enter the OS Username and password
6)Clickn on Continue. 
7)Click add button to view the tables that are available for export.Enter Ganesh(Schema Name) in the schema fields and Click Go
8)Select the desired tables using the checkboxes. Click select
9)Click Next.Enter 4 for the maximum number of threads in export job and select appropriate directory object if one exists or click create Directory object to create one. Change the name of the export log file to MyBackupLog_<Today's date in mmddyyformat>.log, Click on Next.
Select the approrpiate directory object and enter the desired file name. %D in the fiel name indicates the data of the job in YYMMDD format.
Click Next
10)Enter the job name. select immediately options under the start sections, Click Next
11)Select the submit job button to submit the export job
12)the job is successfully create as indicated in the screen
13)Click the EXPORT_GANESHSCHEMA link to see the job summary .The screen indicates that the job has finished
14)Click the Step:Export link to see the log 


Data Pump Import
----------------
1)To export a schema using EM, open a web browser point it to 
	https://<hostname>:1158/em
2)Ensure that DBConsole service is switched on for the Enterprise manager to be served
3)Log in SYSTEM/<password>
4)Click on Data Movement link. this displays the login entry screen
5)Click Import from files link
6)Select the directory object and enter the dump file name.Set the Import type to tables and enter the host username and password.Click Continue.
7)The import files are read 
8)Enter <schemaName> ganesh in the schema field and click Go
9)Select the desired tables using check boxes and click select
10)Click next
11)Use this to screen to re-map Schemas if required. this is useful if tables from one schema need to be imported to another schema
12)Click next. Select appropriate directory object and enter the log file name in the log file.
13)Click on Next, Enter job name and select Immediately options under the start sections.Click Next.
14)Click Submit job, after some time
15)Click IMPORT_GANESHSCHEMA link to view the job_summary. this displays a screen 
16)Click step:Import link to view the log.
17)the import has completely successfully.

